---
title: "GLM_Project"
author: "MC"
date: "2025-09-29"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Data

```{r}
x <- c(1.6907, 1.7242, 1.7552, 1.7842, 1.8113, 1.8369, 1.8610, 1.8839)
n1 <- c(6 ,13,18,28,52,53,61,60)
n0 <- c(59,60,62,56,63,59,62,60) - n1
```

# Theoretical Part

## Logit Model
$$log(\frac{p}{1-p})=\beta_1+\beta_2x \Leftrightarrow p(y=1|x) = \frac{e^{\beta_1 + \beta_2x}}{1+e^{\beta_1 + \beta_2x}}$$


### Likelihood function


#### Likelihood for logit model

$$l((x_k,n_k)_k; \beta)=\prod_{j=1}^k(\prod_{i=1}^{n_j}(\pi_j)^{y_{ij}}(1-\pi_j)^{1-y_{ij}})$$

$$l((x_k,n_k)_k; \beta)=\prod_{j=1}^k(\prod_{i=1}^{n_j}(\frac{e^{\beta_1+\beta_2x_j}}{1 + e^{\beta_1+\beta_2x_j}})^{y_{ij}}(\frac{1}{1 + e^{\beta_1+\beta_2x_j}})^{1-y_{ij}})$$ 

$$l((x_k,n_k)_k; \beta)=\prod_{j=1}^k(\frac{e^{\beta_1+\beta_2x_j}}{1 + e^{\beta_1+\beta_2x_j}})^{n_j^1}(\frac{1}{1 + e^{\beta_1+\beta_2x_j}})^{n_j^0}$$

$$l((x_k,n_k)_k; \beta) =\prod_{j=1}^k(\frac{e^{\beta_1+\beta_2x_j}}{1 + e^{\beta_1+\beta_2x_j}})^{n_j^1}(\frac{1}{1 + e^{\beta_1+\beta_2x_j}})^{n_j^0}$$

#### Log-Likelihood


$$L((x_k,n_k)_k; \beta) = \sum_{j=1}^klog((\frac{e^{\beta_1+\beta_2x_j}}{1 + e^{\beta_1+\beta_2x_j}})^{n_j^1}) + log((\frac{1}{1 + e^{\beta_1+\beta_2x_j}})^{n_j^0})$$

$$L((x_k,n_k)_k; \beta) = -(\sum_{j=1}^kn_j^1log(1 + e^{-(\beta_1+\beta_2x_j)}) + {n_j^0}((\beta_1+\beta_2x_j)+log(1 + e^{-(\beta_1+\beta_2x_j)}))$$


### Score and Fisher Information


#### Score Vector U


$$U(\beta) = \sum_{j=1}^k(n^1_j(1-\pi_j) -n^0_j\pi_j, x_j(n^1_j(1-\pi_j) -n^0_j\pi_j)$$

#### Fisher Information


$$I_{11}=-\frac{\partial^2 L}{\partial \beta_1^2} = \frac{\partial L}{\partial \beta_1}$$
$$I_{22}=-\frac{\partial^2 L}{\partial \beta_2^2} = \sum_{j=1}^k x_j^2(\frac{n^1_j(1-\pi_j)}{\pi_j} -\frac{n^0_j\pi_j}{(1-\pi_j)})$$

$$I_{12}=-\frac{\partial^2 L}{\partial \beta_1 \beta_2} = \sum_{j=1}^k x_j(\frac{n^1_j(1-\pi_j)}{\pi_j} -\frac{n^0_j\pi_j}{(1-\pi_j)})$$

$$I_{21}=I_{12}$$

$$I(\beta)^{-1} =  \frac{\begin{bmatrix}\sum_{j=1}^k x_j^2(\frac{n^1_j(1-\pi_j)}{\pi_j} -\frac{n^0_j\pi_j}{(1-\pi_j)} &-\sum_{j=1}^k x_j(\frac{n^1_j(1-\pi_j)}{\pi_j} -\frac{n^0_j\pi_j}{(1-\pi_j)})\\-\sum_{j=1}^k x_j(\frac{n^1_j(1-\pi_j)}{\pi_j} -\frac{n^0_j\pi_j}{(1-\pi_j)})& \sum_{j=1}^k(\frac{n^1_j(1-\pi_j)}{\pi_j} -\frac{n^0_j\pi_j}{(1-\pi_j)}) 
\end{bmatrix}}{(\sum_{j=1}^k x_j^2(\frac{n^1_j(1-\pi_j)}{\pi_j} -\frac{n^0_j\pi_j}{(1-\pi_j)}))(\sum_{j=1}^k(\frac{n^1_j(1-\pi_j)}{\pi_j} -\frac{n^0_j\pi_j}{(1-\pi_j)})) - (\sum_{j=1}^k x_j(\frac{n^1_j(1-\pi_j)}{\pi_j} -\frac{n^0_j\pi_j}{(1-\pi_j)}))^2}$$


### Newton-Raphson Algorithm


#### Recurrence relation for Beta estimation


$$ \beta_{n+1} = \beta_n + (I(\beta_n))^{-1}U(\beta_n), n \ge 1$$

#### Initial values


### Adjustments Tests


#### Deviance D


#### Pearson Statistics


#### Deviance Residuals


#### Pearson Residuals


## Probit Model


$$p=\phi(\beta_1+\beta_2x)$$

### Likelihood function


#### Likelihood for logit model


$$\prod_{j=1}^k(\Phi(x_j\beta)^{n^1_j}(1 - \Phi(x_j\beta))^{n^0_j}$$

#### Log-Likelihood


$$L(X,\beta_1, \beta_2) = \sum_{j=1}^k (n^1_jlog(\Phi(x_j\beta)) + n^0_jlog(1- \Phi(x_j\beta)))$$

$$L(X,\beta_1, \beta_2) = \sum_{j=1}^k (n^1_jlog(\int_0^{\beta_0 + \beta_1x_j}\frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}}dx) + n^0_jlog(1- \int_0^{\beta_0 + \beta_1x_j}\frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}}dx))$$

### Score and Fisher Information


#### Score Vector U


$$U(\beta) = (\sum_{j=1}^n \frac{e^{-\frac{1}{2}}(n^1_j -n^0_j)}{\sqrt{2\pi}\Phi(\beta_1+\beta_2x_j)}, \sum_{j=1}^n\frac{e^{-\frac{-x_j^2}{2}}(n^1_j -n^0_j)}{\sqrt{2\pi}\Phi(\beta_1 + \beta_2 x_j)})$$

#### Fisher Information


$$I(\beta) =  \begin{bmatrix} \sum_{j=1}^n\frac{e^{-1}(n^1_j - n^0_j)}{2\pi\Phi(\beta_1 + \beta_2 x_j)^2} & \sum_{j=1}^n\frac{e^{-\frac{1}{2}(x_j^2+1)}(n^1_j - n^0_j)}{2\pi\Phi(\beta_1 + \beta_2 x_j)^2}\\
\sum_{j=1}^n\frac{e^{-\frac{1}{2}(x_j^2+1)}(n^1_j - n^0_j)}{2\pi\Phi(\beta_1 + \beta_2 x_j)^2} & \sum_{j=1}^n\frac{e^{-x_j^2}(n^1_j - n^0_j)}{2\pi\Phi(\beta_1 + \beta_2 x_j)^2}
\end{bmatrix}$$

$$I(\beta)^{-1} =  \frac{\begin{bmatrix} \sum_{j=1}^n\frac{e^{-1}(n^1_j - n^0_j)}{2\pi\Phi(\beta_1 + \beta_2 x_j)^2} & -\sum_{j=1}^n\frac{e^{-\frac{1}{2}(x_j^2+1)}(n^1_j - n^0_j)}{2\pi\Phi(\beta_1 + \beta_2 x_j)^2}\\
-\sum_{j=1}^n\frac{e^{-\frac{1}{2}(x_j^2+1)}(n^1_j - n^0_j)}{2\pi\Phi(\beta_1 + \beta_2 x_j)^2} & \sum_{j=1}^n\frac{e^{-x_j^2}(n^1_j - n^0_j)}{2\pi\Phi(\beta_1 + \beta_2 x_j)^2}
\end{bmatrix}}{(\sum_{j=1}^n\frac{e^{-1}(n^1_j - n^0_j)}{2\pi\Phi(\beta_1 + \beta_2 x_j)^2})( \sum_{j=1}^n\frac{e^{-x_j^2}(n^1_j - n^0_j)}{2\pi\Phi(\beta_1 + \beta_2 x_j)^2}) - (\sum_{j=1}^n\frac{e^{-\frac{1}{2}(x_j^2+1)}(n^1_j - n^0_j)}{2\pi\Phi(\beta_1 + \beta_2 x_j)^2})^2}$$


### Newton-Raphson Algorithm


#### Recurrence relation for Beta estimation


$$ \beta_{n+1} = \beta_n + (I(\beta_n))^{-1}U(\beta_n), n \ge 1$$

#### Initial values



### Adjustments Tests


#### Deviance D


#### Pearson Statistics


#### Deviance Residuals


#### Pearson Residuals


## Cloglog Model


$$log(\frac{p}{1-p})=\beta_1+\beta_2x \Leftrightarrow p(y=1|x) = \frac{e^{\beta_1 + \beta_2x}}{1+e^{\beta_1 + \beta_2x}}$$

### Likelihood function


#### Likelihood for logit model


$$l((x_k,n_k)_k; \beta)= \prod_{j=1}^k (1 - e^{-e^{(\beta_1 + \beta_2 x_j)}})^{n^1_j}(e^{-e^{(\beta_1 + \beta_2 x_j)}})^{n^0_j}$$

#### Log-Likelihood


$$L(X,\beta_1, \beta_2) = log(\prod_{j=1}^k (1 - e^{-e^{(\beta_1 + \beta_2 x_j)}})^{n^1_j}(e^{-e^{(\beta_1 + \beta_2 x_j)}})^{n^0_j}) = \sum_{j=1}^k (n^1_j log(\pi_j) + n^0_jlog(1-\pi_j))$$

### Score and Fisher Information


#### Score Vector U


$$U(\beta) = (\sum_{j=1}^k\frac{e^{-e^{(\beta_1 + \beta_2 x_j)}+(\beta_1 + \beta_2 x_j)}}{(1 - e^{-e^{\beta_1 + \beta_2 x_j}})}(n^1_j-n^0_j), \sum_{j=1}^k\frac{x_je^{-e^{(\beta_1 + \beta_2 x_j)}+(\beta_1 + \beta_2 x_j)}}{(1 - e^{-e^{\beta_1 + \beta_2 x_j}})}(n^1_j-n^0_j))$$

#### Fisher Information


$$I(\beta) =  \begin{bmatrix} \sum_{j=1}^k\frac{(e^{-e^{(\beta_1 + \beta_2 x_j)}+(\beta_1 + \beta_2 x_j)})(e^{-e^{\beta_1 + \beta_2 x_j}} + e^{\beta_1 + \beta_2 x_j} - 1)}{(1 - e^{-e^{\beta_1 + \beta_2 x_j}})^2}(n^1_j-n^0_j) & \sum_{j=1}^k\frac{x_j(e^{-e^{(\beta_1 + \beta_2 x_j)}+(\beta_1 + \beta_2 x_j)})(e^{-e^{\beta_1 + \beta_2 x_j}} + e^{\beta_1 + \beta_2 x_j} - 1)}{(1 - e^{-e^{\beta_1 + \beta_2 x_j}})^2}(n^1_j-n^0_j)\\
\sum_{j=1}^k\frac{x_j(e^{-e^{(\beta_1 + \beta_2 x_j)}+(\beta_1 + \beta_2 x_j)})(e^{-e^{\beta_1 + \beta_2 x_j}} + e^{\beta_1 + \beta_2 x_j} - 1)}{(1 - e^{-e^{\beta_1 + \beta_2 x_j}})^2}(n^1_j-n^0_j) & \sum_{j=1}^k\frac{x_j^2(e^{-e^{(\beta_1 + \beta_2 x_j)}+(\beta_1 + \beta_2 x_j)})(e^{-e^{\beta_1 + \beta_2 x_j}} + e^{\beta_1 + \beta_2 x_j} - 1)}{(1 - e^{-e^{\beta_1 + \beta_2 x_j}})^2}(n^1_j-n^0_j)
\end{bmatrix}$$


$$I(\beta) =  \begin{bmatrix} \sum_{i=1}^n\frac{(e^{-\mu_j(\beta)}\mu_j(\beta))(e^{-\mu_j(\beta)} + \mu_j(\beta) - 1)}{(1 - e^{-\mu_j(\beta)})^2}(n^1_j - n^0_j) & \sum_{i=1}^n\frac{x_j(e^{-\mu_j(\beta)}\mu_j(\beta))(e^{-\mu_j(\beta)} + \mu_j(\beta) - 1)}{(1 - e^{-\mu_j(\beta)})^2}(n^1_j - n^0_j)\\
\sum_{i=1}^n\frac{x_j(e^{-\mu_j(\beta)}\mu_j(\beta))(e^{-\mu_j(\beta)} + \mu_j(\beta) - 1)}{(1 - e^{-\mu_j(\beta)})^2}(n^1_j - n^0_j) & \sum_{i=1}^n\frac{x_j^2(e^{-\mu_j(\beta)}\mu_j(\beta))(e^{-\mu_j(\beta)} + \mu_j(\beta) - 1)}{(1 - e^{-\mu_j(\beta)})^2}(n^1_j - n^0_j)
\end{bmatrix}$$

$$I(\beta)^{-1} =  \frac{\begin{bmatrix} \sum_{j=1}^k\frac{x_j^2(e^{-\mu_j(\beta)}\mu_j(\beta))(e^{-\mu_j(\beta)} + \mu_j(\beta) - 1)}{(1 - e^{-\mu_j(\beta)})^2}(n^1_j - n^0_j) & -\sum_{j=1}^k\frac{x_j(e^{-\mu_j(\beta)}\mu_j(\beta))(e^{-\mu_j(\beta)} + \mu_j(\beta) - 1)}{(1 - e^{-\mu_j(\beta)})^2}(n^1_j - n^0_j)\\
-\sum_{j=1}^k\frac{x_j(e^{-\mu_j(\beta)}\mu_j(\beta))(e^{-\mu_j(\beta)} + \mu_j(\beta) - 1)}{(1 - e^{-\mu_j(\beta)})^2}(n^1_j - n^0_j) &  \sum_{j=1}^k\frac{(e^{-\mu_j(\beta)}\mu_j(\beta))(e^{-\mu_j(\beta)} + \mu_j(\beta) - 1)}{(1 - e^{-\mu_j(\beta)})^2}(n^1_j - n^0_j)
\end{bmatrix}}{(\sum_{j=1}^k\frac{x_j^2(e^{-\mu_j(\beta)}\mu_j(\beta))(e^{-\mu_j(\beta)} + \mu_j(\beta) - 1)(n^1_j - n^0_j)}{(1 - e^{-\mu_j(\beta)})^2})(\sum_{j=1}^k\frac{(e^{-\mu_j(\beta)}\mu_j(\beta))(e^{-\mu_j(\beta)} + \mu_j(\beta) - 1)(n^1_j - n^0_j)}{(1 - e^{-\mu_j(\beta)})^2}) - (\sum_{j=1}^k\frac{x_j(e^{-\mu_j(\beta)}\mu_j(\beta))(e^{-\mu_j(\beta)} + \mu_j(\beta) - 1)(n^1_j - n^0_j)}{(1 - e^{-\mu_j(\beta)})^2})^2}$$

### Newton-Raphson Algorithm


#### Recurrence relation for Beta estimation


$$ \beta_{n+1} = \beta_n + (I(\beta_n))^{-1}U(\beta_n), n \ge 1$$

#### Initial values


### Adjustments Tests


#### Deviance D


#### Pearson Statistics


#### Deviance Residuals


#### Pearson Residuals



# Pratical Part


## Logit Model


### Data visualization


#### Plot


```{r}

```


#### Comments


```{r}

```


### Parameter Estimation


#### Newton-Raphson Algorithm Implementation up to Convergence

```{r}
library(ggplot2)
library(tidyr)
library(dplyr)

##data

x <- c(1.6907, 1.7242, 1.7552, 1.7842, 1.8113, 1.8369, 1.8610, 1.8839)
x_scaled <- (x-mean(x))/sd(x)
n1 <- c(6 ,13,18,28,52,53,61,60)
n0 <- c(59,60,62,56,63,59,62,60) - n1

## general

logistic <- function(z) {
  return(ifelse(z >= 0, 
                1 / (1 + exp(-z)), 
                exp(z) / (1 + exp(z))))
}

prob <- function(beta) {
  z <- beta[1] + beta[2]*x_scaled
  return(logistic(z))
}


log_lik <- function(beta) {
  p <- prob(beta)
  return(sum(n1*log(p+eps)+ n0*log(1-p+eps)))
}

## Score

U <- function(beta) {
  U <- matrix(nrow=2, ncol=1)
  p <- prob(beta)
  U[1] <- sum(n1*(1-p)-n0*p)
  U[2] <- sum(x*(n1*(1-p)-n0*p))
  return(U)
}


stoch_U <- function(beta, index) {
  G <- matrix(nrow=2, ncol=1)
  p <- prob(beta)
  w1 <- n1*(1-p)-n0*p
  w2 <- x*(n1*(1-p)-n0*p)
  G[1] <- sum(w1[index])
  G[2] <- sum(w2[index])
  return(G)
}

##Fisher Information

stoch_I <- function(beta, index) {
  com <- matrix(ncol=2, nrow=2)
  p <- prob(beta)
  v1 <- x^2*(n1*(1-p)/(p+eps) - (n0*p)/(1-p+eps))
  v2 <- -x*n1*(1-p)/(p+eps) - n0*p/(1-p+eps)
  v3 <- v2
  v4 <- n1*(1-p)/(p+eps) - n0*p/(1-p+eps)
  com[1,1] <- sum(v1[index])
  com[1,2] <- sum(v2[index])
  com[2,1] <- com[1,2]
  com[2,2] <- sum(v3[index])
  det <- com[1,1]*com[2,2] - com[1,2]^2
  if (abs(det) < 1e-6) {
    warning("Déterminant trop petit, inversion instable")
    det <- diag(2)*1e6
  }
  return(com/(det+eps))
}

I <- function(beta) {
  com <- matrix(ncol=2, nrow=2)
  p <- prob(beta)
  com[1,1] <- sum(x^2*(n1*(1-p)/(p+eps) - (n0*p)/(1-p+eps)))
  com[1,2] <- -sum(x*(n1*(1-p)/(p+eps) - n0*p/(1-p+eps)))
  com[2,1] <- com[1,2]
  com[2,2] <- sum(n1*(1-p)/(p+eps) - n0*p/(1-p+eps))
  det <- (com[1,1]*com[2,2] - com[1,2]^2)
  if (abs(det) < 1e-6) {
    warning("Déterminant trop petit, inversion instable")
    det <- diag(2)*1e6
  }
  return(com/(det+eps))
}

## EMV Algo

EMV_algo <- function(beta, n) {
  ll_old <- log_lik(beta)
  step <- 1
  if (n > 20000) {
    param <- sqrt(lambda/n)
  } else {
    param <- lambda/sqrt(n)
  }
  for (i in 1:10) {
    beta_check <- beta + param*step*(I(beta)%*%U(beta))
    ll_new <- log_lik(beta_check)
    if (all(is.finite(beta_check)) && ll_new >= ll_old) {
      break
    }
    step <- step / 2
  }
  beta_star <- beta_check
  return(beta_star)
}

EMV_algo_stoch <- function(beta, n, index) {
  ll_old <- log_lik(beta)
  step <- 1
  if (n > 20000) {
    param <- sqrt(lambda/n)
  } else {
    param <- lambda/sqrt(n)
  }
  for (i in 1:10) {
    beta_check <- beta + param*step*(stoch_I(beta, index)%*%stoch_U(beta, index))
    ll_new <- log_lik(beta_check)
    if (all(is.finite(beta_check)) && ll_new >= ll_old) {
      break
    }
    step <- step / 2
  }
  beta_star <- beta_check
  return(beta_star)
}

##EMV call

resu_EMV_Batch <- function(beta_H1, beta_H2) {
  for (j in 2:N) {
  resu <- EMV_algo(c(beta_H1[j-1],beta_H2[j-1]),j)
  beta_H1[j] <- resu[1]
  beta_H2[j] <- resu[2]
  
  if (sqrt(sum((resu - c(beta_H1[j-1],beta_H2[j-1]))^2))< nu) {
    message("Convergence atteinte à l'itération ", j)
    break
  }
  }
  return(c(beta_H1, beta_H2))
}

resu_EMV_SG <- function(beta_H1S, beta_H2S) {
  for (j in 2:N) {
  index <- sample(length(x), floor(3*length(x)/4))
  resuS <- EMV_algo_stoch(c(beta_H1S[j-1],beta_H2S[j-1]), j, index)
  beta_H1S[j] <- resuS[1]
  beta_H2S[j] <- resuS[2]
  if (sqrt(sum((resuS - c(beta_H1S[j-1],beta_H2S[j-1]))^2))< nu) {
    message("Convergence atteinte à l'itération ", j)
    break
    }
  }
 return(c(beta_H1S, beta_H2S)) 
}

####EMV calling

##params
eps <- 1e-8
lambda <- 1e8
nu <- 1e-6
N <- 10000
beta_1_init <- -40
beta_2_init <- 40

beta_H1S <- rep(NA, N)
beta_H1S[1] <- beta_1_init
beta_H2S <- rep(NA, N)
beta_H2S[1] <- beta_2_init

beta_H1 <- rep(NA, N)
beta_H1[1] <- beta_1_init
beta_H2 <- rep(NA, N)
beta_H2[1] <- beta_2_init

EMV_output <- resu_EMV_Batch(beta_H1, beta_H2)
beta_H1 <- EMV_output[1:N]
beta_H2 <- EMV_output[(N+1):(2*N)]
EMV_output_sg <- resu_EMV_SG(beta_H1S, beta_H2S)
beta_H1S <- EMV_output_sg[1:N]
beta_H2S <- EMV_output_sg[(N+1):(2*N)]

###Plot

df <- data.frame(
  iter = 1:N,
  beta_H1 = beta_H1,
  beta_H2 = beta_H2,
  beta_H1S = beta_H1S,
  beta_H2S = beta_H2S
)

df_long <- df %>%
  pivot_longer(cols = -iter, names_to = c("param", "method"), 
               names_pattern = "(beta_H1|beta_H2)(S?)",
               values_to = "value") %>%
  mutate(
    param = recode(param, beta_H1 = "Beta 1", beta_H2 = "Beta 2"),
    method = ifelse(method == "S", "Stochastic", "Batch")
  )

ggplot(df_long, aes(x = iter, y = value, color = method)) +
  geom_line() +
  facet_wrap(~param, scales = "free_y") +
  theme_minimal() +
  labs(
    title = "Convergence of Parameters Over Iterations",
    x = "Iteration",
    y = "Parameter Value",
    color = "Algorithm"
  )

```


#### MLE Estimators

```{r}
c(c(beta_H1[N], beta_H2[N]), c(beta_H1S[N], beta_H2S[N]))
```

### Predictions


#### Computing linear predicted values

```{r}

```

#### Computing predicted proportions

```{r}

```

#### Computing quantities Yj for each Xj

```{r}

```

### Result Visualisation


#### Plotting observed proportions vs predicted proportions depending on dose X 


```{r}

```


#### Commenting on adjustment quality


```{r}

```


### Diagnostics 


#### Computing Deviance


```{r}

```


#### Computing Pearson Statistics


```{r}

```


## Probit Model


### Data visualization


#### Plot


```{r}

```


#### Comments


```{r}

```


### Parameter Estimation


#### Newton-Raphson Algorithm Implementation up to Convergence

```{r}

```


#### MLE Estimators

```{r}
```

### Predictions


#### Computing linear predicted values


```{r}

```


#### Computing predicted proportions


```{r}

```


#### Computing quantities Yj for each Xj


```{r}

```


### Result Visualisation


#### Plotting observed proportions vs predicted proportions depending on dose X 


```{r}

```

#### Commenting on adjustment quality


```{r}

```


### Diagnostics 


#### Computing Deviance


```{r}

```


#### Computing Pearson Statistics


```{r}

```


## Cloglog Model


### Data visualization


#### Plot


```{r}

```


#### Comments


```{r}

```


### Parameter Estimation


#### Newton-Raphson Algorithm Implementation up to Convergence

```{r}

```


#### MLE Estimators


```{r}
c(c(beta_H1[N], beta_H2[N]), c(beta_H1S[N], beta_H2S[N]))
```


### Predictions


#### Computing linear predicted values


```{r}

```


#### Computing predicted proportions


```{r}

```


#### Computing quantities Yj for each Xj


```{r}

```


### Result Visualisation


#### Plotting observed proportions vs predicted proportions depending on dose X 


```{r}

```

#### Commenting on adjustment quality


### Diagnostics 


#### Computing Deviance

```{r}

```


#### Computing Pearson Statistics


```{r}

```


# Model Comparison


### Synthesis of results


#### Table of Deviance and Pearson Statistics Deviance for Logit, Probit and Cloglog Models


#### Estimators Comparisons 


### Model Selection


#### Selection criterion based on deviance


#### Pros and Cons of each model


### Conclusion


#### Which model most adapted to data?


#### Discussion on Model Selection


