---
title: "GLM_Project"
author: "MC"
date: "2025-09-29"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Data

```{r cars}
x <- c(1.6907, 1.7242, 1.7552, 1.7842, 1.8113, 1.8369, 1.8610, 1.8839)
n1 <- c(6 ,13,18,28,52,53,61,60)
n0 <- c(59,60,62,56,63,59,62,60) - n1
```


# Logit Model

$$log(\frac{p}{1-p})=\beta_0+\beta_1x$$
$$p(y=1|x) = \frac{e^{\beta_0 + \beta_1x}}{1+e^{\beta_0 + \beta_1x}}$$


## Likelihood function:


$$l((x_k,n_k)_k; \beta)=\prod_{j=1}^k(\prod_{i=1}^{n_j}(\pi_j)^{y_{ij}}(1-\pi_j)^{1-y_{ij}})$$


$$l((x_k,n_k)_k; \beta)=\prod_{j=1}^k(\prod_{i=1}^{n_j}(\frac{e^{\beta_0+\beta_1x_j}}{1 + e^{\beta_0+\beta_1x_j}})^{y_{ij}}(\frac{1}{1 + e^{\beta_0+\beta_1x_j}})^{1-y_{ij}})$$ 


$$l((x_k,n_k)_k; \beta)=\prod_{j=1}^k(\frac{e^{\beta_0+\beta_1x_j}}{1 + e^{\beta_0+\beta_1x_j}})^{n_j^1}(\frac{1}{1 + e^{\beta_0+\beta_1x_j}})^{n_j^0}$$


$$l((x_k,n_k)_k; \beta) =\prod_{j=1}^k(\frac{e^{\beta_0+\beta_1x_j}}{1 + e^{\beta_0+\beta_1x_j}})^{n_j^1}(\frac{1}{1 + e^{\beta_0+\beta_1x_j}})^{n_j^0}$$
$$L((x_k,n_k)_k; \beta) = \sum_{j=1}^klog((\frac{e^{\beta_0+\beta_1x_j}}{1 + e^{\beta_0+\beta_1x_j}})^{n_j^1}(\frac{1}{1 + e^{\beta_0+\beta_1x_j}})^{n_j^0})$$

## Log-likelihood function:

$$L((x_k,n_k)_k; \beta) = \sum_{j=1}^klog((\frac{e^{\beta_0+\beta_1x_j}}{1 + e^{\beta_0+\beta_1x_j}})^{n_j^1}) + log((\frac{1}{1 + e^{\beta_0+\beta_1x_j}})^{n_j^0})$$

$$L((x_k,n_k)_k; \beta) = -(\sum_{j=1}^kn_j^1log(1 + e^{-(\beta_0+\beta_1x_j)}) + {n_j^0}((\beta_0+\beta_1x_j)+log(1 + e^{-(\beta_0+\beta_1x_j)}))$$

## Score function:

$$U(\beta) = \sum_{j=1}^k(n^1_j(1-\pi_j) -n^0_j\pi_j, x_j(n^1_j(1-\pi_j) -n^0_j\pi_j)$$


## Fisher Information Matrix:

$$I_{11}=-\frac{\partial^2 L}{\partial \beta_0^2} = \frac{\partial L}{\partial \beta_0})$$
$$I_{22}=-\frac{\partial^2 L}{\partial \beta_1^2} = \sum_{j=1}^k x_j^2(\frac{n^1_j(1-\pi_j)}{\pi_j} -\frac{n^0_j\pi_j}{(1-\pi_j)})$$

$$I_{12}=-\frac{\partial^2 L}{\partial \beta_0 \beta_1} = \sum_{j=1}^k x_j(\frac{n^1_j(1-\pi_j)}{\pi_j} -\frac{n^0_j\pi_j}{(1-\pi_j)})$$

$$I_{21}=I_{12}$$

$$I(\beta)^{-1} =  \frac{\begin{bmatrix}\sum_{j=1}^k x_j^2(\frac{n^1_j(1-\pi_j)}{\pi_j} -\frac{n^0_j\pi_j}{(1-\pi_j)} &-\sum_{j=1}^k x_j(\frac{n^1_j(1-\pi_j)}{\pi_j} -\frac{n^0_j\pi_j}{(1-\pi_j)})\\-\sum_{j=1}^k x_j(\frac{n^1_j(1-\pi_j)}{\pi_j} -\frac{n^0_j\pi_j}{(1-\pi_j)})& \sum_{j=1}^k(\frac{n^1_j(1-\pi_j)}{\pi_j} -\frac{n^0_j\pi_j}{(1-\pi_j)}) 
\end{bmatrix}}{(\sum_{j=1}^k x_j^2(\frac{n^1_j(1-\pi_j)}{\pi_j} -\frac{n^0_j\pi_j}{(1-\pi_j)}))(\sum_{j=1}^k(\frac{n^1_j(1-\pi_j)}{\pi_j} -\frac{n^0_j\pi_j}{(1-\pi_j)})) - (\sum_{j=1}^k x_j(\frac{n^1_j(1-\pi_j)}{\pi_j} -\frac{n^0_j\pi_j}{(1-\pi_j)}))^2}$$


## EMV Algorithm's Toolbox:

$$U(\beta) = \sum_{j=1}^k(n^1_j(1-\pi_j) -n^0_j\pi_j, x_j(n^1_j(1-\pi_j) -n^0_j\pi_j)$$

$$I(\beta)^{-1} =  \frac{\begin{bmatrix}\sum_{j=1}^k x_j^2(\frac{n^1_j(1-\pi_j)}{\pi_j} -\frac{n^0_j\pi_j}{(1-\pi_j)}) &-\sum_{j=1}^k x_j(\frac{n^1_j(1-\pi_j)}{\pi_j} -\frac{n^0_j\pi_j}{(1-\pi_j)})\\-\sum_{j=1}^k x_j(\frac{n^1_j(1-\pi_j)}{\pi_j} -\frac{n^0_j\pi_j}{(1-\pi_j)})& \sum_{j=1}^k(\frac{n^1_j(1-\pi_j)}{\pi_j} -\frac{n^0_j\pi_j}{(1-\pi_j)}) 
\end{bmatrix}}{(\sum_{j=1}^k x_j^2(\frac{n^1_j(1-\pi_j)}{\pi_j} -\frac{n^0_j\pi_j}{(1-\pi_j)}))(\sum_{j=1}^k(\frac{n^1_j(1-\pi_j)}{\pi_j} -\frac{n^0_j\pi_j}{(1-\pi_j)})) - (\sum_{j=1}^k x_j(\frac{n^1_j(1-\pi_j)}{\pi_j} -\frac{n^0_j\pi_j}{(1-\pi_j)}))^2}$$

```{r, echo=FALSE}
x <- c(1.6907, 1.7242, 1.7552, 1.7842, 1.8113, 1.8369, 1.8610, 1.8839)
x_scaled <- (x-mean(x))/sd(x)
n1 <- c(6 ,13,18,28,52,53,61,60)
n0 <- c(59,60,62,56,63,59,62,60) - n1
eps <- 1e-8

logistic <- function(z) {
  return(ifelse(z >= 0, 
                1 / (1 + exp(-z)), 
                exp(z) / (1 + exp(z))))
}

prob <- function(beta) {
  z <- beta[1] + beta[2]*x_scaled
  return(logistic(z))
}

U <- function(beta) {
  U <- matrix(nrow=2, ncol=1)
  p <- prob(beta)
  U[1] <- sum(n1*(1-p)-n0*p)
  U[2] <- sum(x*(n1*(1-p)-n0*p))
  return(U)
}

I <- function(beta) {
  com <- matrix(ncol=2, nrow=2)
  p <- prob(beta)
  com[1,1] <- sum(x^2*(n1*(1-p)/(p+eps) - (n0*p)/(1-p+eps)))
  com[1,2] <- -sum(x*(n1*(1-p)/(p+eps) - n0*p/(1-p+eps)))
  com[2,1] <- com[1,2]
  com[2,2] <- sum(n1*(1-p)/(p+eps) - n0*p/(1-p+eps))
  det <- (com[1,1]*com[2,2] - com[1,2]^2)
  if (abs(det) < 1e-6) {
    warning("DÃ©terminant trop petit, inversion instable")
    det <- diag(2)*1e6
  }
  return(com/(det+eps))
}

log_lik <- function(beta) {
  p <- prob(beta)
  return(sum(n1*log(p+eps)+ n0*log(1-p+eps)))
}

EMV_algo <- function(beta) {
  ll_old <- log_lik(beta)
  step <- 1
  for (i in 1:10) {
    beta_check <- beta + step*(I(beta)%*%U(beta))
    ll_new <- log_lik(beta_check)
    if (all(is.finite(beta_check)) && ll_new >= ll_old) {
      break
    }
    step <- step / 2
  }
  beta_star <- beta_check
  return(beta_star)
}
N <- 10000
beta_H1 <- rep(NA, N)
beta_H1[1] <- -60
beta_H2 <- rep(NA, N)
beta_H2[1] <- 34
for (j in 2:N) {
  resu <- EMV_algo(c(beta_H1[j-1],beta_H2[j-1]))
  beta_H1[j] <- resu[1]
  beta_H2[j] <- resu[2]
}
c(beta_H1[N], beta_H2[N])
```

```{r, echo=FALSE}
x <- c(1.6907, 1.7242, 1.7552, 1.7842, 1.8113, 1.8369, 1.8610, 1.8839)
n1 <- c(6 ,13,18,28,52,53,61,60)
n0 <- c(59,60,62,56,63,59,62,60) - n1
set.seed(123)
prob <- function(beta) {
  return(exp(beta[1]+beta[2]*x)/(1+exp(beta[1]+beta[2]*x)))
}

neg_lk <- function(beta) {
  return(-sum(n1*log(prob(beta))+n0*log(1-prob(beta))))
}
N <- 100
resu1 <- rep(NA, N)
resu2 <- rep(NA, N)
for (j in 1:N) {
  param <- runif(2, -100, 100)
  emv <- nlm(neg_lk, p=param)
  resu1[j] <- emv$estimate[1]
  resu2[j] <- emv$estimate[2]
}
fit <- glm(cbind(n1, n0) ~ x, family = binomial(link = "logit"))
coef(fit)
mean(c(resu1 - coef(fit)[1],resu2-coef(fit)[2]))
var(c(resu1 - coef(fit)[1],resu2-coef(fit)[2]))
```
